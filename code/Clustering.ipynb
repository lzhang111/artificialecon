{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/laurazhang/Documents/artificialecon/code\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "print os.getcwd()\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"../datasets/phd_clustering.csv\" # input\n",
    "housing_df = pandas.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows 6834\n",
      "All cols 67\n",
      "NY rows 804\n",
      "NY cols 67\n"
     ]
    }
   ],
   "source": [
    "# Drop missing values that are negative\n",
    "housing_df[housing_df < 0] = np.nan\n",
    "\n",
    "# Information on housing_df\n",
    "print \"All rows\", len(housing_df)\n",
    "print \"All cols\", len(housing_df.columns)\n",
    "\n",
    "# Subset of rows for NY housing\n",
    "df = housing_df.loc[housing_df['STD_ST'].isin(['NY','DC', 'MA', 'ND','CT'])]\n",
    "\n",
    "# Information on ny_df\n",
    "print \"NY rows\", len(df)\n",
    "print \"NY cols\", len(df.columns)\n",
    "\n",
    "# STD_ST = 'NY' for ny_df because all rows are for NY\n",
    "del df['STD_ST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLKGRP_ID                     3.810595e+11\n",
      "TOTAL_UNITS                   2.391000e+03\n",
      "TOTAL_DWELLING_UNITS          2.391000e+03\n",
      "ACC_UNITS                     2.391000e+03\n",
      "TOTAL_OCCUPIED                2.377000e+03\n",
      "REGULAR_VACANT                2.620000e+02\n",
      "PHA_TOTAL_UNITS               1.746200e+05\n",
      "PCT_OCCUPIED                  1.000000e+02\n",
      "NUMBER_REPORTED               2.354000e+03\n",
      "PCT_REPORTED                  1.000000e+02\n",
      "MONTHS_SINCE_REPORT           5.900000e+01\n",
      "PCT_MOVEIN                    1.000000e+02\n",
      "PEOPLE_PER_UNIT               7.400000e+00\n",
      "PEOPLE_TOTAL                  5.232000e+03\n",
      "RENT_PER_MONTH                1.072000e+03\n",
      "SPENDING_PER_MONTH            8.350000e+02\n",
      "HH_INCOME                     4.439300e+04\n",
      "PERSON_INCOME                 3.068200e+04\n",
      "PCT_LT5K                      4.407000e+01\n",
      "PCT_5K_LT10K                  7.143000e+01\n",
      "PCT_10K_LT15K                 7.674000e+01\n",
      "PCT_15K_LT20K                 4.000000e+01\n",
      "PCT_GE20K                     9.091000e+01\n",
      "PCT_WAGE_MAJOR                1.000000e+02\n",
      "PCT_WELFARE_MAJOR             3.889000e+01\n",
      "PCT_OTHER_MAJOR               1.000000e+02\n",
      "PCT_MEDIAN                    6.562000e+01\n",
      "PCT_LT50_MEDIAN               1.000000e+02\n",
      "PCT_LT30_MEDIAN               1.000000e+02\n",
      "PCT_2ADULTS                   8.833000e+01\n",
      "                                  ...     \n",
      "PCT_LT24_HEAD                 2.857000e+01\n",
      "PCT_AGE25_50                  9.167000e+01\n",
      "PCT_AGE51_61                  5.686000e+01\n",
      "PCT_AGE62PLUS                 1.000000e+02\n",
      "PCT_AGE85PLUS                 3.500000e+01\n",
      "PCT_MINORITY                  1.000000e+02\n",
      "PCT_BLACK                     1.000000e+02\n",
      "PCT_NATIVE_AMERICAN           9.492000e+01\n",
      "PCT_ASIAN                     6.607000e+01\n",
      "PCT_HISPANIC                  9.796000e+01\n",
      "MONTHS_WAITING                2.600000e+02\n",
      "MONTHS_FROM_MOVEIN            1.398000e+03\n",
      "PCT_UTILITY_ALLOW             1.000000e+02\n",
      "AVE_UTIL_ALLOW                3.930000e+02\n",
      "PCT_BED1                      1.000000e+02\n",
      "PCT_BED2                      1.000000e+02\n",
      "PCT_BED3                      1.000000e+02\n",
      "PCT_OVERHOUSED                7.647000e+01\n",
      "TMINORITY                     9.964000e+01\n",
      "TPOVERTY                      7.505000e+01\n",
      "TPCT_OWNSFD                   9.870000e+01\n",
      "STATE2KX                      3.800000e+01\n",
      "SPENDING_PER_MONTH_PREV_YR    1.083000e+03\n",
      "CHLDRN_MBR_CNT                1.669000e+03\n",
      "ELDLY_PRCNT                   1.000000e+02\n",
      "PCT_DISABLED_LT62_ALL         7.708000e+01\n",
      "PCT_LT80_MEDIAN               1.000000e+02\n",
      "MEDIAN_INC_AMNT               4.182600e+04\n",
      "ANNL_EXPNS_AMNT               1.280090e+07\n",
      "ANNL_EXPNS_AMNT_PREV_YR       1.718912e+07\n",
      "dtype: float64\n",
      "cols 66\n"
     ]
    }
   ],
   "source": [
    "print pandas.DataFrame.max(df)\n",
    "\n",
    "print \"cols\", len(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols 41\n",
      "length 804\n"
     ]
    }
   ],
   "source": [
    "ncols = 65 # input\n",
    "#df1 = train_df.ix[:,1:ncols]\n",
    "\n",
    "# only keep necessary features\n",
    "colarray = [1, 7, 12, 14, 15, 16, 18, 19, \\\n",
    "            20, 21, 22, 23, 24, 25, 27, 28, \\\n",
    "            29, 30, 31, 32, 33, 36, 37, 38, 39, \\\n",
    "            40, 41, 42, 43, 44, 45, 46, 47, 48, \\\n",
    "            49, 50, 51, 52, 53, 55, 64]\n",
    "colarray2 = [1, 7, 13, 14, 15, 16, 17, 18, 19]\n",
    "df1 = df.ix[:, colarray]\n",
    "print \"cols\", len(df1.columns)\n",
    "print \"length\", len(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # KMeans Initial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAAH9CAYAAAD74aE/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCUlEQVSISHT+f4g46IskoDijAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1158d1850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import Imputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "df1 = imp.fit_transform(df1)\n",
    "\n",
    "# ncluster = 5 # input\n",
    "\n",
    "range_n_clusters = range(2,50)\n",
    "\n",
    "sil_scores = []\n",
    "for n_cluster in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_cluster, random_state = 0).fit(df1)\n",
    "    cluster_labels = clusterer.fit_predict(df1)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(df1, cluster_labels)\n",
    "    sil_scores.append(silhouette_score(df1, cluster_labels, metric='euclidean'))\n",
    "    # print(\"For n_clusters =\", n_cluster,\n",
    "    #        \"The average silhouette_score is :\", silhouette_avg)\n",
    "    # print \"labels\", cluster_labels\n",
    "\n",
    "plt.plot(sil_scores)\n",
    "plt.ylabel(\"Silhouette\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.title(\"Silhouette Scores for K-Means Clusters\")\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run K-Means with set cluster number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [0 4 0 0 2 2 0 4 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 0 0 2 1 4 1 1 4 1 0 0 0 0 1\n",
      " 4 2 0 4 2 2 0 2 0 2 2 0 2 2 0 0 0 2 4 1 0 1 0 0 3 4 4 2 0 0 0 2 0 0 4 2 2\n",
      " 0 2 1 3 1 1 3 2 0 0 0 0 0 0 4 0 0 4 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 3 0 3 4\n",
      " 1 3 1 3 1 0 0 0 0 0 0 0 4 4 3 4 2 0 0 0 0 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 4 2 0 4 1 2 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 2 0 0 2 0\n",
      " 0 0 2 0 0 0 4 3 0 0 2 2 1 3 1 4 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2\n",
      " 0 0 4 0 0 0 2 1 2 0 3 0 0 0 0 2 0 0 3 1 4 2 0 4 0 0 0 0 2 0 0 2 0 2 2 0 0\n",
      " 0 0 0 2 2 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 3 2 4 0 0 0 0\n",
      " 0 0 0 0 0 0 0 4 2 0 0 0 0 2 0 0 2 3 0 0 0 4 1 2 0 0 0 0 2 0 0 2 0 0 4 2 3\n",
      " 3 2 0 0 2 3 4 4 0 0 4 4 0 2 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 2 0 0\n",
      " 0 0 2 2 0 0 1 2 2 0 0 0 0 0 0 2 2 0 2 0 0 2 0 0 2 0 2 2 0 0 0 2 0 0 0 1 2\n",
      " 1 0 0 0 0 0 0 2 0 2 0 2 0 0 2 0 0 0 1 1 0 0 0 0 4 1 0 0 0 0 2 0 2 0 2 4 1\n",
      " 4 0 0 2 0 2 0 0 2 0 2 0 2 0 0 0 0 2 0 0 0 0 2 0 0 2 0 0 0 2 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 2 0 0 0 0 1 4 3 0\n",
      " 0 0 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 2 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0\n",
      " 0 2 0 1 1 4 0 0 4 0 0 2 0 0 0 0 0 2 4 0 0 2 2 2 0 0 0 0 0 2 0 0 0 2 2 0 2\n",
      " 2 1 0 4 4 2 0 0 0 2 0 2 0 0 0 0 0 0 0 0 2 2 0 2 2 0 0 1 1 0 0 3 1 0 0 0 0\n",
      " 0 0 0 0 0 0 2 0 2 2 0 0 0 0 2 2 2 0 0 2 0 0 0 1 3 3 4 4 2 0 0 2 0 2 0 0 0\n",
      " 0 0 2 0 0 0 1 4 1 0 0 2 0 0 0 1 2 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2\n",
      " 0 1 3 4 0 0 0 0 2 0 2 0 0 0 1 0 0 0 0 2 3 1 2 2 1 0 0 2 0 2 0 0 0 0 0 2 2\n",
      " 0 2 2 1 0 0 0 0 0 0 0 2 0 0 0 2 4 0 1 3 0 0 0 4 4 4 2 0 0 2 0 4 4 2 0 0 0\n",
      " 0 0 2 0 0 4 0 0 0 0 0 0 4 4 2 2 0 0 2 2 0 0 3 1 4 4 2]\n",
      "clusters [[  1.12203125e+02   9.59968945e+01   1.93515625e+00   3.84447266e+02\n",
      "    3.51078125e+02   1.75768613e+04   5.23111328e+00   2.11825781e+01\n",
      "    2.86795313e+01   1.54997070e+01   2.94066406e+01   2.66167578e+01\n",
      "    4.29535156e+00   6.62034766e+01   9.13643750e+01   7.14027344e+01\n",
      "    4.52158203e+00   2.55411523e+01   6.93300195e+01   2.63873633e+01\n",
      "    5.10518750e+01   2.87197266e+00   3.41303320e+01   1.96301172e+01\n",
      "    4.33679688e+01   5.33402344e+00   5.94560547e+01   3.43729492e+01\n",
      "    1.16619141e+00   3.00298828e+00   2.09136719e+01   2.05468750e+01\n",
      "    1.56117187e+02   4.23781641e+01   5.87714844e+01   5.76304102e+01\n",
      "    1.96409375e+01   2.27287109e+01   8.18847656e+00   2.38146875e+01\n",
      "    3.98577170e+05]\n",
      " [  1.43860417e+03   9.85770833e+01   2.24583333e+00   5.14270833e+02\n",
      "    4.49625000e+02   2.32501458e+04   3.76875000e+00   2.45104167e+01\n",
      "    1.93552083e+01   1.28766667e+01   3.94887500e+01   4.15502083e+01\n",
      "    6.95583333e+00   5.14018750e+01   8.20812500e+01   6.50281250e+01\n",
      "    4.12708333e+00   3.05310417e+01   7.73758333e+01   3.04556250e+01\n",
      "    2.42000000e+01   1.94729167e+00   3.70845833e+01   2.39585417e+01\n",
      "    3.70104167e+01   3.98854167e+00   9.46756250e+01   4.80850000e+01\n",
      "    1.66041667e-01   4.55937500e+00   4.18662500e+01   4.37500000e-01\n",
      "    2.20145833e+02   2.97333333e+00   9.43750000e+00   2.37475000e+01\n",
      "    5.18962500e+01   2.43558333e+01   2.55797917e+01   3.89625000e+01\n",
      "    7.48277850e+06]\n",
      " [  2.83176101e+02   8.60972956e+01   2.00759924e+00   3.78503080e+02\n",
      "    4.28489224e+02   1.73878416e+04   6.59503625e+00   2.56835355e+01\n",
      "    2.70539729e+01   1.34358754e+01   2.72326339e+01   2.97280907e+01\n",
      "    5.86366366e+00   6.15372524e+01   9.09575573e+01   7.54677895e+01\n",
      "    3.86739014e+00   2.75187047e+01   7.00770952e+01   2.79980572e+01\n",
      "    4.49013574e+01   2.70596378e+00   3.65043902e+01   2.33108310e+01\n",
      "    3.74793938e+01   3.84793698e+00   8.04168296e+01   4.28417350e+01\n",
      "    4.31249156e-01   3.49856208e+00   3.36451023e+01   2.02116804e+01\n",
      "    1.48229596e+02   2.64913705e+01   3.74555558e+01   4.79396735e+01\n",
      "    2.82962591e+01   2.37642561e+01   1.37227236e+01   3.30538123e+01\n",
      "    1.64239871e+06]\n",
      " [  1.93907407e+03   9.87577778e+01   2.29629630e+00   5.18370370e+02\n",
      "    4.49000000e+02   2.32952593e+04   3.28703704e+00   2.39874074e+01\n",
      "    1.91685185e+01   1.36014815e+01   3.99562963e+01   4.18077778e+01\n",
      "    6.85592593e+00   5.13022222e+01   8.27833333e+01   6.50651852e+01\n",
      "    4.60259259e+00   3.05692593e+01   7.70400000e+01   3.03270370e+01\n",
      "    2.35718519e+01   1.75037037e+00   3.67674074e+01   2.43874074e+01\n",
      "    3.70937037e+01   4.05407407e+00   9.58644444e+01   3.84485185e+01\n",
      "    1.63703704e-01   6.75814815e+00   5.04970370e+01  -1.06581410e-14\n",
      "    2.27555556e+02   3.70370370e-03   1.28148148e+01   1.99025926e+01\n",
      "    5.24648148e+01   2.76318519e+01   2.75948148e+01   3.90225926e+01\n",
      "    1.02230525e+07]\n",
      " [  8.84844828e+02   9.78818966e+01   2.25862069e+00   4.86344828e+02\n",
      "    4.51896552e+02   2.19635345e+04   4.34793103e+00   2.49431034e+01\n",
      "    2.09750000e+01   1.28822414e+01   3.68513793e+01   3.80731034e+01\n",
      "    7.43103448e+00   5.40401724e+01   8.47615517e+01   6.80525862e+01\n",
      "    3.91896552e+00   3.10391379e+01   7.66725862e+01   3.09294828e+01\n",
      "    2.57996552e+01   2.06431034e+00   3.64289655e+01   2.20627586e+01\n",
      "    3.94441379e+01   4.54275862e+00   9.31725862e+01   4.80272414e+01\n",
      "    1.98965517e-01   3.51758621e+00   4.14298276e+01   1.48275862e+00\n",
      "    1.98310345e+02   1.26160345e+01   1.51034483e+01   3.32601724e+01\n",
      "    3.85543103e+01   2.81851724e+01   2.10141379e+01   3.54600000e+01\n",
      "    4.50791259e+06]]\n",
      "        BLKGRP_ID  Labels\n",
      "0    110010072002       1\n",
      "1    110010078042       5\n",
      "2    110010069001       1\n",
      "3    110010072002       1\n",
      "4    110010099071       3\n",
      "5    110010035002       3\n",
      "6    250092216002       1\n",
      "7    360470449002       5\n",
      "8    250251401071       1\n",
      "9    250250924004       1\n",
      "10   250250610003       1\n",
      "11   250138116004       1\n",
      "12   250056508004       3\n",
      "13   110010035002       1\n",
      "14   360593018005       1\n",
      "15   250138014011       1\n",
      "16   250173732001       1\n",
      "17   250235021021       1\n",
      "18    90034057001       1\n",
      "19    90091504002       1\n",
      "20   360610066008       1\n",
      "21   360594123021       1\n",
      "22    90035107003       3\n",
      "23   360010026004       1\n",
      "24   360830401001       1\n",
      "25   361190063002       3\n",
      "26   360470343001       2\n",
      "27   360470908002       5\n",
      "28   360470185013       2\n",
      "29   360610299001       2\n",
      "..            ...     ...\n",
      "774  360010002003       1\n",
      "775  360550024004       1\n",
      "776  360450621001       1\n",
      "777  360594144004       1\n",
      "778  360894915001       1\n",
      "779  360010002001       3\n",
      "780  250250821003       1\n",
      "781  250251010021       1\n",
      "782  250173550002       5\n",
      "783  250056408003       1\n",
      "784  250056508004       1\n",
      "785  250138113011       1\n",
      "786  110010034001       1\n",
      "787  110010089041       1\n",
      "788  110010099031       1\n",
      "789  360810954003       5\n",
      "790  360610179006       5\n",
      "791  360610093006       3\n",
      "792  360650213012       3\n",
      "793  360010001002       1\n",
      "794   90035004001       1\n",
      "795   90091421001       3\n",
      "796   90091416004       3\n",
      "797   90010215004       1\n",
      "798   90116905002       1\n",
      "799  360810972032       4\n",
      "800  360470342004       2\n",
      "801  360050129012       5\n",
      "802  360610232003       5\n",
      "803  250173119002       3\n",
      "\n",
      "[804 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "cluster_num = 5\n",
    "clusterer5 = KMeans(n_clusters=cluster_num, random_state = 0).fit(df1)\n",
    "cluster_labels5 = clusterer5.fit_predict(df1)\n",
    "print \"labels\", cluster_labels5\n",
    "print \"clusters\", clusterer5.cluster_centers_\n",
    "\n",
    "labels = pandas.DataFrame({'Labels': cluster_labels5})+1\n",
    "block = df.ix[:,0]\n",
    "block = block.reset_index(drop=True)\n",
    "\n",
    "# add labels to dataset\n",
    "combined = pandas.concat([block, labels], axis=1)\n",
    "print(combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read in X data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        BLOCKGROUPID  jobs_idx  schl_idx  households  area_median_income  \\\n",
      "0        10010201001      41.0      68.0       175.0             47317.0   \n",
      "1        10010201002       2.0      68.0       457.0             47317.0   \n",
      "2        10010202001      10.0      68.0       452.0             47317.0   \n",
      "3        10010202002      73.0      68.0       400.0             47317.0   \n",
      "4        10010203001      15.0      68.0       901.0             47317.0   \n",
      "5        10010203002      81.0      68.0       239.0             47317.0   \n",
      "6        10010204001      32.0      68.0       402.0             47317.0   \n",
      "7        10010204002      25.0      92.0       692.0             47317.0   \n",
      "8        10010204003      59.0      79.0       343.0             47317.0   \n",
      "9        10010204004      80.0      79.0       264.0             47317.0   \n",
      "10       10010205001      82.0      92.0       729.0             47317.0   \n",
      "11       10010205002      74.0      92.0      2418.0             47317.0   \n",
      "12       10010205003      27.0      92.0       772.0             47317.0   \n",
      "13       10010206001      28.0      68.0       804.0             47317.0   \n",
      "14       10010206002      87.0      68.0       338.0             47317.0   \n",
      "15       10010207001      67.0      68.0       606.0             47317.0   \n",
      "16       10010207002      79.0      79.0       481.0             47317.0   \n",
      "17       10010208011      34.0      68.0       295.0             47317.0   \n",
      "18       10010208012      32.0      68.0       709.0             47317.0   \n",
      "19       10010208021      21.0      77.0       898.0             47317.0   \n",
      "20       10010208022      51.0      68.0      1127.0             47317.0   \n",
      "21       10010208023      14.0      77.0       728.0             47317.0   \n",
      "22       10010208024      34.0      77.0       906.0             47317.0   \n",
      "23       10010209001      19.0      74.0       488.0             47317.0   \n",
      "24       10010209002       9.0      77.0       620.0             47317.0   \n",
      "25       10010209003      31.0      77.0       550.0             47317.0   \n",
      "26       10010209004      20.0      77.0       481.0             47317.0   \n",
      "27       10010210001       8.0      74.0       333.0             47317.0   \n",
      "28       10010210002      40.0      74.0       930.0             47317.0   \n",
      "29       10010211001      15.0      60.0       899.0             47317.0   \n",
      "...              ...       ...       ...         ...                 ...   \n",
      "220008  721519513002       NaN       2.0         NaN                 NaN   \n",
      "220009  721519513003       NaN      75.0         NaN                 NaN   \n",
      "220010  721537501011       NaN      42.0         NaN                 NaN   \n",
      "220011  721537501012       NaN      42.0         NaN                 NaN   \n",
      "220012  721537501021       NaN      61.0         NaN                 NaN   \n",
      "220013  721537501022       NaN      87.0         NaN                 NaN   \n",
      "220014  721537502011       NaN      91.0         NaN                 NaN   \n",
      "220015  721537502012       NaN      55.0         NaN                 NaN   \n",
      "220016  721537502021       NaN      61.0         NaN                 NaN   \n",
      "220017  721537502022       NaN      61.0         NaN                 NaN   \n",
      "220018  721537502023       NaN      61.0         NaN                 NaN   \n",
      "220019  721537503001       NaN      61.0         NaN                 NaN   \n",
      "220020  721537503002       NaN      61.0         NaN                 NaN   \n",
      "220021  721537504001       NaN      72.0         NaN                 NaN   \n",
      "220022  721537504002       NaN      72.0         NaN                 NaN   \n",
      "220023  721537504003       NaN      90.0         NaN                 NaN   \n",
      "220024  721537504004       NaN      90.0         NaN                 NaN   \n",
      "220025  721537504005       NaN      90.0         NaN                 NaN   \n",
      "220026  721537505011       NaN      68.0         NaN                 NaN   \n",
      "220027  721537505012       NaN      42.0         NaN                 NaN   \n",
      "220028  721537505013       NaN      68.0         NaN                 NaN   \n",
      "220029  721537505021       NaN      68.0         NaN                 NaN   \n",
      "220030  721537505022       NaN      68.0         NaN                 NaN   \n",
      "220031  721537505031       NaN      42.0         NaN                 NaN   \n",
      "220032  721537505032       NaN      68.0         NaN                 NaN   \n",
      "220033  721537506011       NaN      92.0         NaN                 NaN   \n",
      "220034  721537506012       NaN      90.0         NaN                 NaN   \n",
      "220035  721537506013       NaN      90.0         NaN                 NaN   \n",
      "220036  721537506021       NaN      42.0         NaN                 NaN   \n",
      "220037  721537506022       NaN      92.0         NaN                 NaN   \n",
      "\n",
      "        blkgrp_median_income_owners  blkgrp_median_income_renters  \\\n",
      "0                         68354.039                     21061.295   \n",
      "1                         60339.824                     18591.949   \n",
      "2                         46465.355                     24120.818   \n",
      "3                         53316.578                     27677.385   \n",
      "4                         50068.484                     31625.479   \n",
      "5                         44644.555                     28199.484   \n",
      "6                         66973.273                     62759.426   \n",
      "7                         53385.395                     50026.473   \n",
      "8                         52872.109                     49545.484   \n",
      "9                         43486.664                     40750.555   \n",
      "10                        69738.055                     28738.783   \n",
      "11                        89083.586                     36711.000   \n",
      "12                        75080.766                     30940.496   \n",
      "13                        64820.508                     24093.285   \n",
      "14                        42935.602                     15958.833   \n",
      "15                        35575.730                     20068.721   \n",
      "16                        59569.922                     33604.148   \n",
      "17                        49765.180                     14672.393   \n",
      "18                       105964.040                     31241.643   \n",
      "19                        53379.059                     49802.078   \n",
      "20                        78041.148                     72811.531   \n",
      "21                        37644.066                     35121.500   \n",
      "22                        70960.133                     66205.023   \n",
      "23                        39672.777                     24192.033   \n",
      "24                        69984.195                     42675.613   \n",
      "25                        38513.430                     23485.074   \n",
      "26                        68226.578                     41603.832   \n",
      "27                        49388.824                      8249.249   \n",
      "28                        62156.422                     10381.778   \n",
      "29                        46883.977                     23383.695   \n",
      "...                             ...                           ...   \n",
      "220008                          NaN                           NaN   \n",
      "220009                          NaN                           NaN   \n",
      "220010                          NaN                           NaN   \n",
      "220011                          NaN                           NaN   \n",
      "220012                          NaN                           NaN   \n",
      "220013                          NaN                           NaN   \n",
      "220014                          NaN                           NaN   \n",
      "220015                          NaN                           NaN   \n",
      "220016                          NaN                           NaN   \n",
      "220017                          NaN                           NaN   \n",
      "220018                          NaN                           NaN   \n",
      "220019                          NaN                           NaN   \n",
      "220020                          NaN                           NaN   \n",
      "220021                          NaN                           NaN   \n",
      "220022                          NaN                           NaN   \n",
      "220023                          NaN                           NaN   \n",
      "220024                          NaN                           NaN   \n",
      "220025                          NaN                           NaN   \n",
      "220026                          NaN                           NaN   \n",
      "220027                          NaN                           NaN   \n",
      "220028                          NaN                           NaN   \n",
      "220029                          NaN                           NaN   \n",
      "220030                          NaN                           NaN   \n",
      "220031                          NaN                           NaN   \n",
      "220032                          NaN                           NaN   \n",
      "220033                          NaN                           NaN   \n",
      "220034                          NaN                           NaN   \n",
      "220035                          NaN                           NaN   \n",
      "220036                          NaN                           NaN   \n",
      "220037                          NaN                           NaN   \n",
      "\n",
      "        avg_hh_size_owners  avg_hh_size_renters  commuters_per_hh_owners  \\\n",
      "0                     2.95                 2.03                  0.91462   \n",
      "1                     2.91                 2.15                  1.45318   \n",
      "2                     3.02                 2.56                  1.16302   \n",
      "3                     2.38                 1.42                  1.04920   \n",
      "4                     2.79                 2.89                  1.22426   \n",
      "5                     3.02                 1.84                  1.32029   \n",
      "6                     2.62                 4.06                  1.12980   \n",
      "7                     2.50                 2.74                  1.04965   \n",
      "8                     2.17                 3.67                  0.95895   \n",
      "9                     2.20                 2.18                  0.72991   \n",
      "10                    2.72                 1.54                  1.33272   \n",
      "11                    2.99                 2.38                  1.47911   \n",
      "12                    2.71                 3.16                  1.34686   \n",
      "13                    3.04                 4.74                  1.32607   \n",
      "14                    2.89                 1.72                  1.43032   \n",
      "15                    2.50                 2.72                  1.14580   \n",
      "16                    2.64                 3.27                  1.17813   \n",
      "17                    2.97                 2.77                  1.04238   \n",
      "18                    3.16                 4.48                  1.51714   \n",
      "19                    2.57                 3.05                  0.89593   \n",
      "20                    3.07                 4.40                  1.45382   \n",
      "21                    2.80                 1.69                  1.20341   \n",
      "22                    2.92                 3.14                  1.35536   \n",
      "23                    2.41                 3.32                  1.04752   \n",
      "24                    2.56                 4.96                  1.17090   \n",
      "25                    2.57                 3.76                  1.01001   \n",
      "26                    2.68                 1.46                  1.12308   \n",
      "27                    2.25                 2.73                  1.02686   \n",
      "28                    2.36                 2.60                  1.18042   \n",
      "29                    2.78                 2.35                  1.21229   \n",
      "...                    ...                  ...                      ...   \n",
      "220008                 NaN                  NaN                      NaN   \n",
      "220009                 NaN                  NaN                      NaN   \n",
      "220010                 NaN                  NaN                      NaN   \n",
      "220011                 NaN                  NaN                      NaN   \n",
      "220012                 NaN                  NaN                      NaN   \n",
      "220013                 NaN                  NaN                      NaN   \n",
      "220014                 NaN                  NaN                      NaN   \n",
      "220015                 NaN                  NaN                      NaN   \n",
      "220016                 NaN                  NaN                      NaN   \n",
      "220017                 NaN                  NaN                      NaN   \n",
      "220018                 NaN                  NaN                      NaN   \n",
      "220019                 NaN                  NaN                      NaN   \n",
      "220020                 NaN                  NaN                      NaN   \n",
      "220021                 NaN                  NaN                      NaN   \n",
      "220022                 NaN                  NaN                      NaN   \n",
      "220023                 NaN                  NaN                      NaN   \n",
      "220024                 NaN                  NaN                      NaN   \n",
      "220025                 NaN                  NaN                      NaN   \n",
      "220026                 NaN                  NaN                      NaN   \n",
      "220027                 NaN                  NaN                      NaN   \n",
      "220028                 NaN                  NaN                      NaN   \n",
      "220029                 NaN                  NaN                      NaN   \n",
      "220030                 NaN                  NaN                      NaN   \n",
      "220031                 NaN                  NaN                      NaN   \n",
      "220032                 NaN                  NaN                      NaN   \n",
      "220033                 NaN                  NaN                      NaN   \n",
      "220034                 NaN                  NaN                      NaN   \n",
      "220035                 NaN                  NaN                      NaN   \n",
      "220036                 NaN                  NaN                      NaN   \n",
      "220037                 NaN                  NaN                      NaN   \n",
      "\n",
      "             ...         median_rooms_per_owner_unit  \\\n",
      "0            ...                                 5.7   \n",
      "1            ...                                 6.5   \n",
      "2            ...                                 7.2   \n",
      "3            ...                                 6.2   \n",
      "4            ...                                 6.2   \n",
      "5            ...                                 6.7   \n",
      "6            ...                                 7.0   \n",
      "7            ...                                 6.6   \n",
      "8            ...                                 6.2   \n",
      "9            ...                                 6.0   \n",
      "10           ...                                 6.4   \n",
      "11           ...                                 6.8   \n",
      "12           ...                                 6.7   \n",
      "13           ...                                 6.9   \n",
      "14           ...                                 4.7   \n",
      "15           ...                                 5.2   \n",
      "16           ...                                 6.3   \n",
      "17           ...                                 6.3   \n",
      "18           ...                                 8.1   \n",
      "19           ...                                 6.3   \n",
      "20           ...                                 6.5   \n",
      "21           ...                                 5.3   \n",
      "22           ...                                 5.9   \n",
      "23           ...                                 5.7   \n",
      "24           ...                                 6.0   \n",
      "25           ...                                 5.2   \n",
      "26           ...                                 6.1   \n",
      "27           ...                                 5.1   \n",
      "28           ...                                 5.8   \n",
      "29           ...                                 6.1   \n",
      "...          ...                                 ...   \n",
      "220008       ...                                 NaN   \n",
      "220009       ...                                 NaN   \n",
      "220010       ...                                 NaN   \n",
      "220011       ...                                 NaN   \n",
      "220012       ...                                 NaN   \n",
      "220013       ...                                 NaN   \n",
      "220014       ...                                 NaN   \n",
      "220015       ...                                 NaN   \n",
      "220016       ...                                 NaN   \n",
      "220017       ...                                 NaN   \n",
      "220018       ...                                 NaN   \n",
      "220019       ...                                 NaN   \n",
      "220020       ...                                 NaN   \n",
      "220021       ...                                 NaN   \n",
      "220022       ...                                 NaN   \n",
      "220023       ...                                 NaN   \n",
      "220024       ...                                 NaN   \n",
      "220025       ...                                 NaN   \n",
      "220026       ...                                 NaN   \n",
      "220027       ...                                 NaN   \n",
      "220028       ...                                 NaN   \n",
      "220029       ...                                 NaN   \n",
      "220030       ...                                 NaN   \n",
      "220031       ...                                 NaN   \n",
      "220032       ...                                 NaN   \n",
      "220033       ...                                 NaN   \n",
      "220034       ...                                 NaN   \n",
      "220035       ...                                 NaN   \n",
      "220036       ...                                 NaN   \n",
      "220037       ...                                 NaN   \n",
      "\n",
      "        median_rooms_per_renter_unit  pct_detatched_single_family_uni  \\\n",
      "0                                3.9                        66.816139   \n",
      "1                                4.3                        95.209579   \n",
      "2                                4.8                        74.200432   \n",
      "3                                4.3                        77.358490   \n",
      "4                                6.3                        91.977081   \n",
      "5                                4.5                        67.834389   \n",
      "6                                5.9                        96.382980   \n",
      "7                                5.9                        95.724136   \n",
      "8                                5.9                        96.683670   \n",
      "9                                5.6                        79.166672   \n",
      "10                               4.4                        86.008232   \n",
      "11                               4.4                        57.290470   \n",
      "12                               6.4                       100.000000   \n",
      "13                               5.9                        97.965736   \n",
      "14                               3.9                        20.541759   \n",
      "15                               4.3                        34.056992   \n",
      "16                               5.7                        85.207100   \n",
      "17                               4.1                        79.940117   \n",
      "18                               5.5                        74.069321   \n",
      "19                               5.0                        57.142860   \n",
      "20                               6.0                        93.255623   \n",
      "21                               6.9                        62.318840   \n",
      "22                               7.2                        72.200394   \n",
      "23                               4.0                        50.000000   \n",
      "24                               8.0                        55.623100   \n",
      "25                               4.8                        40.156250   \n",
      "26                               5.5                        56.293709   \n",
      "27                               6.0                        53.427898   \n",
      "28                               4.4                        53.876350   \n",
      "29                               4.8                        62.665310   \n",
      "...                              ...                              ...   \n",
      "220008                           NaN                              NaN   \n",
      "220009                           NaN                              NaN   \n",
      "220010                           NaN                              NaN   \n",
      "220011                           NaN                              NaN   \n",
      "220012                           NaN                              NaN   \n",
      "220013                           NaN                              NaN   \n",
      "220014                           NaN                              NaN   \n",
      "220015                           NaN                              NaN   \n",
      "220016                           NaN                              NaN   \n",
      "220017                           NaN                              NaN   \n",
      "220018                           NaN                              NaN   \n",
      "220019                           NaN                              NaN   \n",
      "220020                           NaN                              NaN   \n",
      "220021                           NaN                              NaN   \n",
      "220022                           NaN                              NaN   \n",
      "220023                           NaN                              NaN   \n",
      "220024                           NaN                              NaN   \n",
      "220025                           NaN                              NaN   \n",
      "220026                           NaN                              NaN   \n",
      "220027                           NaN                              NaN   \n",
      "220028                           NaN                              NaN   \n",
      "220029                           NaN                              NaN   \n",
      "220030                           NaN                              NaN   \n",
      "220031                           NaN                              NaN   \n",
      "220032                           NaN                              NaN   \n",
      "220033                           NaN                              NaN   \n",
      "220034                           NaN                              NaN   \n",
      "220035                           NaN                              NaN   \n",
      "220036                           NaN                              NaN   \n",
      "220037                           NaN                              NaN   \n",
      "\n",
      "        median_commute_distance  block_denstiy  employment_access_index  \\\n",
      "0                     13.487200        0.02481               4374.70800   \n",
      "1                     16.575199        0.02253               3148.71240   \n",
      "2                     15.759300        0.02949               4318.71390   \n",
      "3                     12.058300        0.11679               5440.71190   \n",
      "4                     14.392300        0.02303               4896.71680   \n",
      "5                     12.315400        0.05731               6156.71440   \n",
      "6                     12.942200        0.02223               4839.71970   \n",
      "7                     14.021900        0.06584               5848.71920   \n",
      "8                     12.640100        0.14014               5974.71680   \n",
      "9                     12.588900        0.05785               6048.71530   \n",
      "10                    10.265500        0.02908               5794.71920   \n",
      "11                    13.113100        0.03735               5837.72360   \n",
      "12                    10.605400        0.04513               6338.71730   \n",
      "13                    13.960300        0.03296               5271.70650   \n",
      "14                    12.216300        0.04840               3528.70190   \n",
      "15                    12.771400        0.01983               4764.71090   \n",
      "16                    12.966400        0.04457               5722.71240   \n",
      "17                    13.543700        0.00310               1693.68900   \n",
      "18                    15.171700        0.00897               1846.69800   \n",
      "19                    19.495800        0.00310               1267.70920   \n",
      "20                    13.900100        0.01151               2662.72410   \n",
      "21                    17.045401        0.00444               1470.72830   \n",
      "22                    16.334999        0.00754               1635.74050   \n",
      "23                    25.458799        0.00273               1064.72660   \n",
      "24                    21.723801        0.00218               1254.73930   \n",
      "25                    25.166901        0.00536               1128.76590   \n",
      "26                    20.845800        0.00421               1302.75040   \n",
      "27                    29.007999        0.00247                902.66656   \n",
      "28                    26.520399        0.00372               1006.70200   \n",
      "29                    22.744900        0.00238               1029.67400   \n",
      "...                         ...            ...                      ...   \n",
      "220008                      NaN            NaN                      NaN   \n",
      "220009                      NaN            NaN                      NaN   \n",
      "220010                      NaN            NaN                      NaN   \n",
      "220011                      NaN            NaN                      NaN   \n",
      "220012                      NaN            NaN                      NaN   \n",
      "220013                      NaN            NaN                      NaN   \n",
      "220014                      NaN            NaN                      NaN   \n",
      "220015                      NaN            NaN                      NaN   \n",
      "220016                      NaN            NaN                      NaN   \n",
      "220017                      NaN            NaN                      NaN   \n",
      "220018                      NaN            NaN                      NaN   \n",
      "220019                      NaN            NaN                      NaN   \n",
      "220020                      NaN            NaN                      NaN   \n",
      "220021                      NaN            NaN                      NaN   \n",
      "220022                      NaN            NaN                      NaN   \n",
      "220023                      NaN            NaN                      NaN   \n",
      "220024                      NaN            NaN                      NaN   \n",
      "220025                      NaN            NaN                      NaN   \n",
      "220026                      NaN            NaN                      NaN   \n",
      "220027                      NaN            NaN                      NaN   \n",
      "220028                      NaN            NaN                      NaN   \n",
      "220029                      NaN            NaN                      NaN   \n",
      "220030                      NaN            NaN                      NaN   \n",
      "220031                      NaN            NaN                      NaN   \n",
      "220032                      NaN            NaN                      NaN   \n",
      "220033                      NaN            NaN                      NaN   \n",
      "220034                      NaN            NaN                      NaN   \n",
      "220035                      NaN            NaN                      NaN   \n",
      "220036                      NaN            NaN                      NaN   \n",
      "220037                      NaN            NaN                      NaN   \n",
      "\n",
      "        local_job_density  local_retail_job_density  retail_access_index  \\\n",
      "0                 0.48949                   0.01802            398.29529   \n",
      "1                 0.12574                   0.00239            314.29575   \n",
      "2                 0.14806                   0.00474            444.29590   \n",
      "3                 1.52292                   0.11767            630.29572   \n",
      "4                 0.17108                   0.02143            647.29620   \n",
      "5                 0.94170                   0.16640            831.29596   \n",
      "6                 0.08745                   0.00171            717.29657   \n",
      "7                 0.38356                   0.07222           1055.29650   \n",
      "8                 1.16826                   0.32399           1056.29630   \n",
      "9                 0.73445                   0.12798           1110.29610   \n",
      "10                1.16609                   0.39654           1353.29650   \n",
      "11                1.37581                   0.22411           1248.29700   \n",
      "12                0.74099                   0.16185           1435.29630   \n",
      "13                0.49558                   0.01121            500.29510   \n",
      "14                0.24517                   0.00982            345.29462   \n",
      "15                0.19212                   0.02604            744.29559   \n",
      "16                0.84235                   0.20581            931.29578   \n",
      "17                0.02804                   0.00097            195.29318   \n",
      "18                0.00719                   0.00028            203.29416   \n",
      "19                0.00310                   0.00004            148.29539   \n",
      "20                0.16013                   0.01208            357.29703   \n",
      "21                0.00237                   0.00000            176.29749   \n",
      "22                0.00994                   0.00000            193.29881   \n",
      "23                0.00269                   0.00000            125.29728   \n",
      "24                0.00602                   0.00000            136.29868   \n",
      "25                0.00373                   0.00015            134.30157   \n",
      "26                0.01216                   0.00000            147.29988   \n",
      "27                0.00082                   0.00006            108.29072   \n",
      "28                0.00437                   0.00022            117.29459   \n",
      "29                0.00065                   0.00000            120.29154   \n",
      "...                   ...                       ...                  ...   \n",
      "220008                NaN                       NaN                  NaN   \n",
      "220009                NaN                       NaN                  NaN   \n",
      "220010                NaN                       NaN                  NaN   \n",
      "220011                NaN                       NaN                  NaN   \n",
      "220012                NaN                       NaN                  NaN   \n",
      "220013                NaN                       NaN                  NaN   \n",
      "220014                NaN                       NaN                  NaN   \n",
      "220015                NaN                       NaN                  NaN   \n",
      "220016                NaN                       NaN                  NaN   \n",
      "220017                NaN                       NaN                  NaN   \n",
      "220018                NaN                       NaN                  NaN   \n",
      "220019                NaN                       NaN                  NaN   \n",
      "220020                NaN                       NaN                  NaN   \n",
      "220021                NaN                       NaN                  NaN   \n",
      "220022                NaN                       NaN                  NaN   \n",
      "220023                NaN                       NaN                  NaN   \n",
      "220024                NaN                       NaN                  NaN   \n",
      "220025                NaN                       NaN                  NaN   \n",
      "220026                NaN                       NaN                  NaN   \n",
      "220027                NaN                       NaN                  NaN   \n",
      "220028                NaN                       NaN                  NaN   \n",
      "220029                NaN                       NaN                  NaN   \n",
      "220030                NaN                       NaN                  NaN   \n",
      "220031                NaN                       NaN                  NaN   \n",
      "220032                NaN                       NaN                  NaN   \n",
      "220033                NaN                       NaN                  NaN   \n",
      "220034                NaN                       NaN                  NaN   \n",
      "220035                NaN                       NaN                  NaN   \n",
      "220036                NaN                       NaN                  NaN   \n",
      "220037                NaN                       NaN                  NaN   \n",
      "\n",
      "        hh_type1_income  \n",
      "0               47317.0  \n",
      "1               47317.0  \n",
      "2               47317.0  \n",
      "3               47317.0  \n",
      "4               47317.0  \n",
      "5               47317.0  \n",
      "6               47317.0  \n",
      "7               47317.0  \n",
      "8               47317.0  \n",
      "9               47317.0  \n",
      "10              47317.0  \n",
      "11              47317.0  \n",
      "12              47317.0  \n",
      "13              47317.0  \n",
      "14              47317.0  \n",
      "15              47317.0  \n",
      "16              47317.0  \n",
      "17              47317.0  \n",
      "18              47317.0  \n",
      "19              47317.0  \n",
      "20              47317.0  \n",
      "21              47317.0  \n",
      "22              47317.0  \n",
      "23              47317.0  \n",
      "24              47317.0  \n",
      "25              47317.0  \n",
      "26              47317.0  \n",
      "27              47317.0  \n",
      "28              47317.0  \n",
      "29              47317.0  \n",
      "...                 ...  \n",
      "220008              NaN  \n",
      "220009              NaN  \n",
      "220010              NaN  \n",
      "220011              NaN  \n",
      "220012              NaN  \n",
      "220013              NaN  \n",
      "220014              NaN  \n",
      "220015              NaN  \n",
      "220016              NaN  \n",
      "220017              NaN  \n",
      "220018              NaN  \n",
      "220019              NaN  \n",
      "220020              NaN  \n",
      "220021              NaN  \n",
      "220022              NaN  \n",
      "220023              NaN  \n",
      "220024              NaN  \n",
      "220025              NaN  \n",
      "220026              NaN  \n",
      "220027              NaN  \n",
      "220028              NaN  \n",
      "220029              NaN  \n",
      "220030              NaN  \n",
      "220031              NaN  \n",
      "220032              NaN  \n",
      "220033              NaN  \n",
      "220034              NaN  \n",
      "220035              NaN  \n",
      "220036              NaN  \n",
      "220037              NaN  \n",
      "\n",
      "[220038 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "xdata = \"../clean/block_merge.csv\" # input\n",
    "xframe = pandas.read_csv(xdata)\n",
    "xframe = xframe[pandas.notnull(xframe['BLOCKGROUPID'])]\n",
    "xframe = xframe.ix[:, 0:23]\n",
    "print(xframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only including the states listed earlier (NY, DC, MA, etc) by using their state FIPS code\n",
    "xdf = xframe.loc[ ((xframe['BLOCKGROUPID'].astype(int)/10**10).astype(int)).isin([36, 11, 25, 38, 9]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset numbering\n",
    "xdf = xdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BLOCKGROUPID  jobs_idx  schl_idx  households  area_median_income  \\\n",
      "0       90010101011      23.0      55.0       230.0             82614.0   \n",
      "1       90010101012      66.0      55.0       224.0             82614.0   \n",
      "2       90010101013      74.0      67.0       550.0             82614.0   \n",
      "3       90010101014      96.0      67.0       383.0             82614.0   \n",
      "4       90010101021      46.0      88.0       402.0             82614.0   \n",
      "5       90010101022      71.0      67.0       517.0             82614.0   \n",
      "6       90010101023      81.0      67.0       558.0             82614.0   \n",
      "7       90010102011      38.0      88.0       327.0             82614.0   \n",
      "8       90010102012      87.0      72.0       305.0             82614.0   \n",
      "9       90010102013      63.0      88.0       331.0             82614.0   \n",
      "10      90010102021       6.0      89.0       297.0             82614.0   \n",
      "11      90010102022      22.0      78.0       753.0             82614.0   \n",
      "12      90010102023      66.0      89.0       537.0             82614.0   \n",
      "13      90010102024      69.0      78.0       235.0             82614.0   \n",
      "14      90010103001      32.0      72.0       245.0             82614.0   \n",
      "15      90010103002      98.0      81.0       355.0             82614.0   \n",
      "16      90010103003      85.0      67.0       223.0             82614.0   \n",
      "17      90010103004      95.0      56.0       198.0             82614.0   \n",
      "18      90010103005      96.0      41.0       397.0             82614.0   \n",
      "19      90010104001      32.0      55.0       329.0             82614.0   \n",
      "20      90010104002      72.0      41.0       531.0             82614.0   \n",
      "21      90010104003      61.0      45.0       297.0             82614.0   \n",
      "22      90010104004      73.0      39.0       447.0             82614.0   \n",
      "23      90010104005      12.0      39.0       303.0             82614.0   \n",
      "24      90010104006      92.0      39.0       214.0             82614.0   \n",
      "25      90010105001      90.0      56.0       318.0             82614.0   \n",
      "26      90010105002      73.0      56.0       226.0             82614.0   \n",
      "27      90010105003      69.0      56.0       371.0             82614.0   \n",
      "28      90010105004      88.0      56.0       277.0             82614.0   \n",
      "29      90010105005      51.0      39.0       248.0             82614.0   \n",
      "...             ...       ...       ...         ...                 ...   \n",
      "24067  381010112001      46.0      24.0       690.0             52415.0   \n",
      "24068  381010112002      32.0       8.0       285.0             52415.0   \n",
      "24069  381010112003      17.0       8.0       301.0             52415.0   \n",
      "24070  381010113001      84.0      55.0       474.0             52415.0   \n",
      "24071  381010113002      15.0      55.0       605.0             52415.0   \n",
      "24072  381010113003      65.0      74.0       264.0             52415.0   \n",
      "24073  381010113004      27.0      36.0       446.0             52415.0   \n",
      "24074  381039598001      91.0      79.0       531.0             41982.0   \n",
      "24075  381039598002       7.0      79.0       348.0             41982.0   \n",
      "24076  381039600001      37.0      72.0       357.0             41982.0   \n",
      "24077  381039600002      20.0       7.0       428.0             41982.0   \n",
      "24078  381039600003      48.0       7.0       319.0             41982.0   \n",
      "24079  381059534001      36.0      96.0       276.0             69617.0   \n",
      "24080  381059534002      89.0      98.0       525.0             69617.0   \n",
      "24081  381059535001      42.0      75.0       293.0             69617.0   \n",
      "24082  381059535002      94.0      15.0       333.0             69617.0   \n",
      "24083  381059536001      47.0      93.0       634.0             69617.0   \n",
      "24084  381059536002      84.0      80.0       351.0             69617.0   \n",
      "24085  381059537001      57.0      93.0      1052.0             69617.0   \n",
      "24086  381059537002      68.0      88.0      1212.0             69617.0   \n",
      "24087  381059537003      78.0      93.0       378.0             69617.0   \n",
      "24088  381059538001      63.0      88.0       624.0             69617.0   \n",
      "24089  381059538002      31.0      88.0       504.0             69617.0   \n",
      "24090  381059539001      21.0      94.0       419.0             69617.0   \n",
      "24091  381059539002      52.0      94.0       402.0             69617.0   \n",
      "24092  381059539003      10.0      88.0       291.0             69617.0   \n",
      "24093  381059539004      26.0      94.0       372.0             69617.0   \n",
      "24094  381059541001      15.0      88.0       713.0             69617.0   \n",
      "24095  381059541002       5.0      88.0       699.0             69617.0   \n",
      "24096  381059541003      73.0      88.0       664.0             69617.0   \n",
      "\n",
      "       blkgrp_median_income_owners  blkgrp_median_income_renters  \\\n",
      "0                       125341.200                     52145.820   \n",
      "1                       143750.000                     59804.449   \n",
      "2                       290034.130                    120663.180   \n",
      "3                       170945.560                     71118.648   \n",
      "4                       205326.250                    139861.920   \n",
      "5                       251396.310                    171243.440   \n",
      "6                       186703.360                    127176.590   \n",
      "7                       260347.800                    160835.970   \n",
      "8                       163950.630                    101284.350   \n",
      "9                       258046.530                    159414.310   \n",
      "10                      256621.050                    153139.250   \n",
      "11                      187174.550                    111696.880   \n",
      "12                      195234.800                    116506.860   \n",
      "13                      163685.520                     97679.734   \n",
      "14                      121379.880                     60067.848   \n",
      "15                      165222.580                     81764.492   \n",
      "16                      266286.750                    131778.610   \n",
      "17                      142420.800                     70480.469   \n",
      "18                      207530.470                    102701.610   \n",
      "19                      150125.000                    115604.950   \n",
      "20                       96991.039                     74688.719   \n",
      "21                       87612.336                     67466.570   \n",
      "22                       82514.922                     63541.270   \n",
      "23                      115255.700                     88753.570   \n",
      "24                       71009.148                     54681.156   \n",
      "25                      116892.300                     53904.988   \n",
      "26                      176557.980                     81419.875   \n",
      "27                      104548.830                     48212.785   \n",
      "28                       60358.594                     27834.422   \n",
      "29                       97693.320                     45051.363   \n",
      "...                            ...                           ...   \n",
      "24067                    71452.031                     50837.340   \n",
      "24068                    82721.180                     58855.219   \n",
      "24069                    68481.773                     48724.031   \n",
      "24070                    70433.836                     37904.188   \n",
      "24071                    65830.063                     35426.648   \n",
      "24072                    78419.867                     42201.895   \n",
      "24073                    59574.824                     32060.373   \n",
      "24074                    46782.359                     16275.742   \n",
      "24075                    39779.879                     13839.556   \n",
      "24076                    49812.223                     39533.828   \n",
      "24077                    47911.523                     38025.320   \n",
      "24078                    54261.719                     43065.199   \n",
      "24079                    58571.859                     32502.160   \n",
      "24080                    58755.676                     32604.162   \n",
      "24081                    67564.672                     26068.260   \n",
      "24082                   104467.510                     40306.363   \n",
      "24083                    66881.719                     50539.145   \n",
      "24084                    74128.211                     56014.957   \n",
      "24085                    97022.695                     51921.777   \n",
      "24086                   109586.560                     58645.340   \n",
      "24087                    67647.000                     36201.348   \n",
      "24088                    67910.602                     30527.223   \n",
      "24089                    78839.531                     35440.008   \n",
      "24090                   118190.950                     53685.051   \n",
      "24091                    58663.691                     26646.400   \n",
      "24092                    65251.961                     29638.945   \n",
      "24093                   115657.940                     52534.504   \n",
      "24094                    77559.766                     44479.875   \n",
      "24095                    57821.434                     33160.105   \n",
      "24096                    79299.234                     45477.445   \n",
      "\n",
      "       avg_hh_size_owners  avg_hh_size_renters  commuters_per_hh_owners  \\\n",
      "0                    3.10                 5.40                  1.34088   \n",
      "1                    2.90                 1.98                  1.38839   \n",
      "2                    3.01                 2.08                  1.05295   \n",
      "3                    2.81                 1.69                  0.55310   \n",
      "4                    2.58                 3.29                  0.86488   \n",
      "5                    3.61                 3.00                  1.12065   \n",
      "6                    2.94                 2.53                  0.91533   \n",
      "7                    3.37                 2.03                  1.03188   \n",
      "8                    3.16                 2.67                  0.83397   \n",
      "9                    3.55                 3.63                  0.77294   \n",
      "10                   2.90                 1.47                  0.88176   \n",
      "11                   2.80                 2.69                  1.17821   \n",
      "12                   3.01                 3.45                  1.05184   \n",
      "13                   3.57                 3.50                  0.97525   \n",
      "14                   2.97                 2.64                  0.88736   \n",
      "15                   2.99                 2.41                  0.93701   \n",
      "16                   3.05                 3.30                  0.87035   \n",
      "17                   3.16                 2.10                  0.85200   \n",
      "18                   2.38                 2.29                  0.77802   \n",
      "19                   2.52                 2.36                  1.35888   \n",
      "20                   2.44                 2.96                  0.82836   \n",
      "21                   2.47                 1.93                  0.76117   \n",
      "22                   2.32                 2.39                  1.16118   \n",
      "23                   3.40                 2.11                  1.41922   \n",
      "24                   1.86                 3.13                  1.06861   \n",
      "25                   2.76                 2.68                  1.54388   \n",
      "26                   2.56                 2.97                  2.06373   \n",
      "27                   2.59                 2.58                  1.55323   \n",
      "28                   1.92                 2.15                  1.17424   \n",
      "29                   1.58                 1.57                  1.27232   \n",
      "...                   ...                  ...                      ...   \n",
      "24067                2.63                 3.13                  1.52790   \n",
      "24068                2.90                 1.86                  1.42034   \n",
      "24069                3.00                 2.32                  1.35236   \n",
      "24070                2.54                 1.90                  1.55816   \n",
      "24071                2.83                 2.44                  1.52645   \n",
      "24072                3.03                 2.48                  1.27809   \n",
      "24073                2.49                 2.61                  0.97088   \n",
      "24074                2.06                 1.43                  0.82027   \n",
      "24075                2.19                 2.01                  1.14360   \n",
      "24076                2.24                 2.93                  0.80946   \n",
      "24077                2.11                 1.86                  0.72566   \n",
      "24078                2.25                 1.40                  0.89461   \n",
      "24079                2.38                 2.13                  1.00860   \n",
      "24080                2.09                 1.37                  0.97010   \n",
      "24081                2.23                 1.18                  0.91623   \n",
      "24082                2.68                 3.43                  1.49030   \n",
      "24083                2.24                 2.04                  1.18318   \n",
      "24084                2.49                 4.87                  1.30489   \n",
      "24085                2.37                 2.13                  1.58809   \n",
      "24086                3.30                 2.43                  1.47450   \n",
      "24087                2.66                 2.30                  1.79101   \n",
      "24088                2.23                 1.74                  1.06994   \n",
      "24089                2.52                 2.35                  1.34268   \n",
      "24090                2.54                 1.53                  1.73005   \n",
      "24091                1.82                 1.72                  0.97125   \n",
      "24092                2.14                 3.92                  1.08135   \n",
      "24093                2.88                 1.99                  1.38439   \n",
      "24094                2.63                 2.01                  1.43068   \n",
      "24095                1.93                 1.24                  1.31234   \n",
      "24096                2.44                 2.11                  1.25779   \n",
      "\n",
      "        ...    median_rooms_per_renter_unit  pct_detatched_single_family_uni  \\\n",
      "0       ...                             6.1                       100.000000   \n",
      "1       ...                             7.9                       100.000000   \n",
      "2       ...                             8.2                        98.791542   \n",
      "3       ...                             3.2                        85.991379   \n",
      "4       ...                             8.4                        97.399529   \n",
      "5       ...                             9.0                        98.571426   \n",
      "6       ...                             5.4                        94.542252   \n",
      "7       ...                             4.2                       100.000000   \n",
      "8       ...                             5.4                        94.690269   \n",
      "9       ...                             9.0                        94.864052   \n",
      "10      ...                             3.5                        93.265991   \n",
      "11      ...                             5.1                        64.855492   \n",
      "12      ...                             6.0                        88.826820   \n",
      "13      ...                             5.9                       100.000000   \n",
      "14      ...                             6.8                        96.837936   \n",
      "15      ...                             5.6                        74.366203   \n",
      "16      ...                             6.4                        96.860992   \n",
      "17      ...                             4.1                        96.629211   \n",
      "18      ...                             5.9                        49.333328   \n",
      "19      ...                             4.1                        54.941860   \n",
      "20      ...                             5.6                        67.603310   \n",
      "21      ...                             3.2                        48.874599   \n",
      "22      ...                             4.8                        26.621920   \n",
      "23      ...                             2.5                        36.303631   \n",
      "24      ...                             4.2                        39.230770   \n",
      "25      ...                             3.9                        20.520229   \n",
      "26      ...                             4.2                         4.044120   \n",
      "27      ...                             4.5                        14.824800   \n",
      "28      ...                             4.1                         8.143320   \n",
      "29      ...                             4.7                        21.299641   \n",
      "...     ...                             ...                              ...   \n",
      "24067   ...                             6.2                        80.945557   \n",
      "24068   ...                             6.4                        89.197533   \n",
      "24069   ...                             5.4                        82.163742   \n",
      "24070   ...                             4.1                        78.323112   \n",
      "24071   ...                             4.7                        75.438599   \n",
      "24072   ...                             4.2                        86.219078   \n",
      "24073   ...                             5.0                        83.009712   \n",
      "24074   ...                             4.1                        79.264214   \n",
      "24075   ...                             4.4                        63.275429   \n",
      "24076   ...                             7.6                        96.000000   \n",
      "24077   ...                             5.3                        91.891891   \n",
      "24078   ...                             5.9                        93.367348   \n",
      "24079   ...                             5.6                        90.296501   \n",
      "24080   ...                             3.0                        71.951218   \n",
      "24081   ...                             4.3                        82.528740   \n",
      "24082   ...                             4.5                        75.433533   \n",
      "24083   ...                             5.4                        81.567490   \n",
      "24084   ...                             8.2                        83.856499   \n",
      "24085   ...                             4.6                        31.381121   \n",
      "24086   ...                             4.3                        50.000000   \n",
      "24087   ...                             5.1                        54.911839   \n",
      "24088   ...                             3.7                        26.976740   \n",
      "24089   ...                             3.9                        83.304649   \n",
      "24090   ...                             4.8                        68.019089   \n",
      "24091   ...                             4.5                        57.019440   \n",
      "24092   ...                             4.3                       100.000000   \n",
      "24093   ...                             3.9                        47.663551   \n",
      "24094   ...                             4.4                        89.340813   \n",
      "24095   ...                             3.4                        77.163460   \n",
      "24096   ...                             4.2                        54.716980   \n",
      "\n",
      "       median_commute_distance  block_denstiy  employment_access_index  \\\n",
      "0                     7.528100        0.03082              29313.96900   \n",
      "1                     7.822610        0.04515              31737.85500   \n",
      "2                    16.280001        0.01465              21287.13900   \n",
      "3                     9.715750        0.01477              23224.00600   \n",
      "4                     8.589580        0.01888              19470.49200   \n",
      "5                    19.248100        0.02075              20048.92600   \n",
      "6                    14.568300        0.01609              16982.18400   \n",
      "7                    16.189199        0.02979              24525.90400   \n",
      "8                    10.441800        0.02413              27261.66600   \n",
      "9                    18.533701        0.02894              26053.90800   \n",
      "10                    5.207940        0.01635              23067.74400   \n",
      "11                    9.556680        0.02258              31309.63700   \n",
      "12                    7.714750        0.03942              31655.02300   \n",
      "13                    5.445160        0.02047              28079.52900   \n",
      "14                    7.859810        0.04625              36607.24600   \n",
      "15                    7.250420        0.06548              40589.08200   \n",
      "16                   11.198400        0.03779              37105.65200   \n",
      "17                   15.346400        0.02947              40549.51200   \n",
      "18                   12.733100        0.03215              42026.35900   \n",
      "19                    6.830290        0.06700              35445.07800   \n",
      "20                    6.480450        0.04015              37003.60200   \n",
      "21                    6.854770        0.11611              40257.67600   \n",
      "22                    6.811940        0.26671              41724.68000   \n",
      "23                    8.728880        0.17731              41932.19900   \n",
      "24                    6.480730        0.15253              42020.81300   \n",
      "25                    5.472850        0.17366              45951.24200   \n",
      "26                    6.402110        0.15229              46469.71900   \n",
      "27                    5.717410        0.04975              45731.03500   \n",
      "28                    5.989990        0.19484              46105.94500   \n",
      "29                    6.355320        0.09459              43812.25000   \n",
      "...                        ...            ...                      ...   \n",
      "24067                 5.864030        0.00905               1353.38320   \n",
      "24068                16.344200        0.00254                294.38788   \n",
      "24069                14.389200        0.00513                329.39493   \n",
      "24070                10.200400        0.00286               1295.36870   \n",
      "24071                 9.582151        0.00713                687.36139   \n",
      "24072                22.022100        0.00224                222.33746   \n",
      "24073                29.296900        0.00266                204.35359   \n",
      "24074                 0.607760        0.09021               1048.50310   \n",
      "24075                 1.016140        0.05462               1050.50370   \n",
      "24076                21.709101        0.00253                199.52441   \n",
      "24077                32.742298        0.00208                198.53545   \n",
      "24078                16.796499        0.00348                461.53250   \n",
      "24079                26.893700        0.00218                144.22430   \n",
      "24080                16.486099        0.01047                671.24536   \n",
      "24081                30.393000        0.00234                153.18239   \n",
      "24082                13.201900        0.00420                620.18396   \n",
      "24083                24.712900        0.00346                203.22896   \n",
      "24084                16.486799        0.00263                386.20807   \n",
      "24085                 2.466370        0.02276               4601.19730   \n",
      "24086                 2.093530        0.03563               4478.19630   \n",
      "24087                 1.976650        0.13416               5361.19920   \n",
      "24088                 1.288660        0.31392               7529.19820   \n",
      "24089                 1.429370        0.24093               7203.19780   \n",
      "24090                 1.408190        0.34109               6664.19970   \n",
      "24091                 1.287180        0.34647               7278.19870   \n",
      "24092                 1.171530        0.38096               7704.19870   \n",
      "24093                 1.287280        0.12948               7210.19920   \n",
      "24094                 1.914850        0.39902               6085.19730   \n",
      "24095                 1.717260        0.63319               6796.19820   \n",
      "24096                 1.793910        0.11232               6686.19920   \n",
      "\n",
      "       local_job_density  local_retail_job_density  retail_access_index  \\\n",
      "0                0.67306                   0.02626          2719.922100   \n",
      "1                0.51102                   0.03815          2984.908400   \n",
      "2                0.09996                   0.00928          2060.397700   \n",
      "3                0.64396                   0.01355          2138.156300   \n",
      "4                0.08963                   0.00000          1918.117900   \n",
      "5                0.10280                   0.00664          1964.713400   \n",
      "6                0.11295                   0.00709          1678.086400   \n",
      "7                0.22443                   0.01682          2413.481900   \n",
      "8                0.42036                   0.02781          2694.680700   \n",
      "9                0.39626                   0.02071          2574.595200   \n",
      "10               0.10863                   0.00000          2272.144800   \n",
      "11               0.29765                   0.04801          3098.789800   \n",
      "12               0.19612                   0.02654          2968.061300   \n",
      "13               0.21761                   0.00503          2753.778600   \n",
      "14               1.67769                   0.02971          3922.405500   \n",
      "15               3.38970                   0.35579          4367.272900   \n",
      "16               1.53706                   0.03038          3987.224900   \n",
      "17               1.24107                   0.19818          4299.095700   \n",
      "18               2.20267                   0.28753          4340.964400   \n",
      "19               1.08371                   0.01738          3319.819600   \n",
      "20               1.10831                   0.01858          3612.878700   \n",
      "21               2.59483                   0.01186          4054.773400   \n",
      "22               2.42802                   0.15672          4407.772900   \n",
      "23               2.93415                   0.18510          4426.832000   \n",
      "24               4.14228                   0.38304          4524.788600   \n",
      "25              18.13353                   2.18526          4815.063500   \n",
      "26              13.58037                   1.27190          4903.004400   \n",
      "27               4.96636                   0.90695          4857.926800   \n",
      "28               4.85694                   0.86207          4869.916500   \n",
      "29               2.86728                   0.28022          4666.837400   \n",
      "...                  ...                       ...                  ...   \n",
      "24067            0.01223                   0.00060           218.150090   \n",
      "24068            0.00024                   0.00000            41.150600   \n",
      "24069            0.00184                   0.00003            45.151360   \n",
      "24070            0.00781                   0.00366           265.148500   \n",
      "24071            0.00592                   0.00008            82.147720   \n",
      "24072            0.00174                   0.00031            28.145121   \n",
      "24073            0.00041                   0.00004            26.146879   \n",
      "24074            0.15411                   0.01989           125.163100   \n",
      "24075            0.13860                   0.01510           125.163160   \n",
      "24076            0.00055                   0.00009            22.165421   \n",
      "24077            0.00022                   0.00002            22.166620   \n",
      "24078            0.00292                   0.00027            50.166302   \n",
      "24079            0.00053                   0.00000            15.132840   \n",
      "24080            0.02070                   0.00156            56.135132   \n",
      "24081            0.00022                   0.00005            16.128300   \n",
      "24082            0.01753                   0.00014            36.128471   \n",
      "24083            0.00188                   0.00008            21.133350   \n",
      "24084            0.00091                   0.00000            40.131088   \n",
      "24085            0.12092                   0.02579           577.129940   \n",
      "24086            0.20935                   0.00447           450.129820   \n",
      "24087            2.33163                   0.43537           618.130130   \n",
      "24088            4.76883                   0.75764           846.130000   \n",
      "24089            3.88995                   0.39640           859.129940   \n",
      "24090            0.92416                   0.08024           774.130190   \n",
      "24091            3.07047                   0.45706           807.130070   \n",
      "24092            3.57331                   0.54853           867.130070   \n",
      "24093            0.44181                   0.02972           845.130130   \n",
      "24094            0.89651                   0.05938           681.129940   \n",
      "24095            4.35984                   0.57129           751.130000   \n",
      "24096            1.08532                   0.15255           755.130130   \n",
      "\n",
      "       hh_type1_income  Labels  \n",
      "0              82614.0     0.0  \n",
      "1              82614.0     0.0  \n",
      "2              82614.0     0.0  \n",
      "3              82614.0     0.0  \n",
      "4              82614.0     0.0  \n",
      "5              82614.0     0.0  \n",
      "6              82614.0     0.0  \n",
      "7              82614.0     0.0  \n",
      "8              82614.0     0.0  \n",
      "9              82614.0     0.0  \n",
      "10             82614.0     0.0  \n",
      "11             82614.0     0.0  \n",
      "12             82614.0     0.0  \n",
      "13             82614.0     0.0  \n",
      "14             82614.0     0.0  \n",
      "15             82614.0     0.0  \n",
      "16             82614.0     0.0  \n",
      "17             82614.0     0.0  \n",
      "18             82614.0     0.0  \n",
      "19             82614.0     0.0  \n",
      "20             82614.0     0.0  \n",
      "21             82614.0     0.0  \n",
      "22             82614.0     0.0  \n",
      "23             82614.0     0.0  \n",
      "24             82614.0     0.0  \n",
      "25             82614.0     1.0  \n",
      "26             82614.0     0.0  \n",
      "27             82614.0     0.0  \n",
      "28             82614.0     0.0  \n",
      "29             82614.0     0.0  \n",
      "...                ...     ...  \n",
      "24067          52415.0     0.0  \n",
      "24068          52415.0     0.0  \n",
      "24069          52415.0     0.0  \n",
      "24070          52415.0     0.0  \n",
      "24071          52415.0     0.0  \n",
      "24072          52415.0     0.0  \n",
      "24073          52415.0     0.0  \n",
      "24074          41982.0     0.0  \n",
      "24075          41982.0     0.0  \n",
      "24076          41982.0     0.0  \n",
      "24077          41982.0     0.0  \n",
      "24078          41982.0     0.0  \n",
      "24079          69617.0     0.0  \n",
      "24080          69617.0     0.0  \n",
      "24081          69617.0     0.0  \n",
      "24082          69617.0     0.0  \n",
      "24083          69617.0     0.0  \n",
      "24084          69617.0     0.0  \n",
      "24085          69617.0     0.0  \n",
      "24086          69617.0     0.0  \n",
      "24087          69617.0     0.0  \n",
      "24088          69617.0     1.0  \n",
      "24089          69617.0     0.0  \n",
      "24090          69617.0     0.0  \n",
      "24091          69617.0     0.0  \n",
      "24092          69617.0     0.0  \n",
      "24093          69617.0     0.0  \n",
      "24094          69617.0     0.0  \n",
      "24095          69617.0     0.0  \n",
      "24096          69617.0     0.0  \n",
      "\n",
      "[24097 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge X data with labels (gen during Kmeans) by BLOCKGROUPID\n",
    "combined = combined.rename(columns={'BLKGRP_ID': 'BLOCKGROUPID'})\n",
    "combined[['BLOCKGROUPID']] = combined[['BLOCKGROUPID']].apply(pandas.to_numeric)\n",
    "xdf[['BLOCKGROUPID']] = xdf[['BLOCKGROUPID']].apply(pandas.to_numeric)\n",
    "new_xdf = pandas.merge(xdf, combined, on='BLOCKGROUPID', how=\"left\")\n",
    "\n",
    "# replace NaN with 0s for labels\n",
    "new_xdf['Labels'].fillna(0, inplace=True)\n",
    "print(new_xdf)\n",
    "\n",
    "# export to csv\n",
    "new_xdf.to_csv(\"../clean/labeled_block.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data in housing and non-housing\n",
    "X_hashouse = new_xdf[new_xdf['Labels'] != 0]\n",
    "X_hashouse = X_hashouse.reset_index(drop=True)\n",
    "X_nohouse = new_xdf[new_xdf['Labels'] == 0]\n",
    "\n",
    "# get a random sampe of (N housing) number of non housing data points\n",
    "X_nohousehalf = X_nohouse.sample(n=len(X_hashouse.index), random_state = 0)\n",
    "X_nohousehalf = X_nohousehalf.reset_index(drop=True)\n",
    "\n",
    "# separate data not in train test from non housing data\n",
    "X_excl_nohousing = X_nohouse[~X_nohouse['BLOCKGROUPID'].isin(X_nohousehalf['BLOCKGROUPID'])].dropna(how = 'all')\n",
    "X_excl_nohousing.to_csv(\"../clean/block_excl_nohouse\")\n",
    "\n",
    "# concatenate housing and non housing data (50/50 each)\n",
    "X_half_half = pandas.concat([X_nohousehalf, X_hashouse])\n",
    "X_half_half.to_csv(\"../clean/block_half_half.csv\")\n",
    "# print X_half_half\n",
    "\n",
    "# get y labels\n",
    "y = X_half_half['Labels']\n",
    "\n",
    "# impute X data\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "X = imp.fit_transform(X_half_half)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "np.savetxt(\"half_half_Xtrain.csv\", X_train, delimiter=\",\")\n",
    "np.savetxt(\"half_half_Xtest.csv\", X_test, delimiter=\",\")\n",
    "np.savetxt(\"half_half_ytrain.csv\", y_train, delimiter=\",\")\n",
    "np.savetxt(\"half_half_ytest.csv\", y_test, delimiter=\",\")\n",
    "\n",
    "# scale and process data\n",
    "X_scale_train = preprocessing.scale(X_train) \n",
    "X_scale_test = preprocessing.scale(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n",
      "[CV] .......... multi_class=ovr, C=0.01, score=0.674797, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n",
      "[CV] .......... multi_class=ovr, C=0.01, score=0.601626, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n",
      "[CV] .......... multi_class=ovr, C=0.01, score=0.634146, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n",
      "[CV] .......... multi_class=ovr, C=0.01, score=0.626016, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n",
      "[CV] .......... multi_class=ovr, C=0.01, score=0.661157, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n",
      "[CV] .......... multi_class=ovr, C=0.01, score=0.625000, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n",
      "[CV] .......... multi_class=ovr, C=0.01, score=0.608333, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .......... multi_class=ovr, C=0.01, score=0.635593, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n",
      "[CV] .......... multi_class=ovr, C=0.01, score=0.567797, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.01 .........................................\n",
      "[CV] .......... multi_class=ovr, C=0.01, score=0.589744, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.699187, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.658537, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.658537, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.626016, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.677686, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.641667, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.616667, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.644068, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.576271, total=   0.0s\n",
      "[CV] multi_class=multinomial, C=0.01 .................................\n",
      "[CV] .. multi_class=multinomial, C=0.01, score=0.623932, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.699187, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.658537, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.666667, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.658537, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.694215, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.641667, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.608333, total=   0.0s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.635593, total=   0.1s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.635593, total=   0.1s\n",
      "[CV] multi_class=ovr, C=0.1 ..........................................\n",
      "[CV] ........... multi_class=ovr, C=0.1, score=0.649573, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.715447, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.682927, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.666667, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.642276, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.710744, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.683333, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.608333, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.652542, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.644068, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=0.1 ..................................\n",
      "[CV] ... multi_class=multinomial, C=0.1, score=0.649573, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.699187, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.658537, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.674797, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.666667, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.702479, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.650000, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.608333, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.635593, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.618644, total=   0.1s\n",
      "[CV] multi_class=ovr, C=1 ............................................\n",
      "[CV] ............. multi_class=ovr, C=1, score=0.675214, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.707317, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.658537, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.682927, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.658537, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.719008, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.708333, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.616667, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.644068, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.635593, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=1 ....................................\n",
      "[CV] ..... multi_class=multinomial, C=1, score=0.649573, total=   0.1s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.707317, total=   0.2s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.666667, total=   0.2s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.674797, total=   0.1s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.666667, total=   0.1s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.694215, total=   0.2s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.650000, total=   0.1s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.616667, total=   0.1s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.644068, total=   0.1s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.644068, total=   0.1s\n",
      "[CV] multi_class=ovr, C=10 ...........................................\n",
      "[CV] ............ multi_class=ovr, C=10, score=0.683761, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.699187, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.666667, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.674797, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.650407, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.719008, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.691667, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.616667, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.644068, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.627119, total=   0.1s\n",
      "[CV] multi_class=multinomial, C=10 ...................................\n",
      "[CV] .... multi_class=multinomial, C=10, score=0.658120, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'multi_class': ['ovr', 'multinomial'], 'C': [0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    " \n",
    "# Use logistic regress to fit model\n",
    "searchmod = LogisticRegression(penalty = 'l2', dual=False, tol=0.0001, solver='lbfgs', random_state = 0)\n",
    "\n",
    "priors = [10**exp for exp in range(-2, 2, 1)]\n",
    "params = {'C':priors, 'multi_class': ['ovr', 'multinomial']}\n",
    "\n",
    "search = GridSearchCV(searchmod, param_grid = params, cv = 10, verbose = 3)\n",
    "search.fit(X_scale_train[:, 1:-1], y_train) # go back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict using fitted estimate\n",
    "ypred = pandas.DataFrame({'pred_log': pandas.Series(search.predict(X_scale_test[:,1:-1]))})\n",
    "x_test_df = pandas.DataFrame(X_test)\n",
    "test_final = pandas.concat([x_test_df, ypred], axis=1)\n",
    "test_final.to_csv(\"../clean/pred_block.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n",
      "[CV]  max_features=auto, n_estimators=5, score=0.626016, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n",
      "[CV]  max_features=auto, n_estimators=5, score=0.650407, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n",
      "[CV]  max_features=auto, n_estimators=5, score=0.666667, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n",
      "[CV]  max_features=auto, n_estimators=5, score=0.617886, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_features=auto, n_estimators=5, score=0.661157, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n",
      "[CV]  max_features=auto, n_estimators=5, score=0.683333, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n",
      "[CV]  max_features=auto, n_estimators=5, score=0.558333, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n",
      "[CV]  max_features=auto, n_estimators=5, score=0.661017, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n",
      "[CV]  max_features=auto, n_estimators=5, score=0.635593, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=5 ...............................\n",
      "[CV]  max_features=auto, n_estimators=5, score=0.649573, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.642276, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.658537, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.658537, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.650407, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.669421, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.683333, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.583333, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.652542, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.635593, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=6 ...............................\n",
      "[CV]  max_features=auto, n_estimators=6, score=0.641026, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.658537, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.642276, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.674797, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.609756, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.652893, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.683333, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.608333, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.686441, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.627119, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=7 ...............................\n",
      "[CV]  max_features=auto, n_estimators=7, score=0.666667, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.650407, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.674797, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.666667, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.617886, total=   0.0s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.636364, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.675000, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.600000, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.661017, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.652542, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=8 ...............................\n",
      "[CV]  max_features=auto, n_estimators=8, score=0.641026, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.650407, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.699187, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.707317, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.609756, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.661157, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.675000, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.583333, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.686441, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.627119, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=9 ...............................\n",
      "[CV]  max_features=auto, n_estimators=9, score=0.658120, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.642276, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.658537, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.715447, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.617886, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.652893, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.675000, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.600000, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.661017, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.644068, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=10 ..............................\n",
      "[CV]  max_features=auto, n_estimators=10, score=0.641026, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.691057, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.666667, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.707317, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.617886, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.661157, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.700000, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.600000, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.686441, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.610169, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=11 ..............................\n",
      "[CV]  max_features=auto, n_estimators=11, score=0.683761, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.682927, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.666667, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.715447, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.626016, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.669421, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.691667, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.608333, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.652542, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.627119, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=12 ..............................\n",
      "[CV]  max_features=auto, n_estimators=12, score=0.658120, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.674797, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.658537, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.723577, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.626016, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.685950, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.675000, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.600000, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.635593, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.627119, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=13 ..............................\n",
      "[CV]  max_features=auto, n_estimators=13, score=0.666667, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.674797, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.666667, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.715447, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.642276, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.661157, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.666667, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.583333, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.644068, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.652542, total=   0.1s\n",
      "[CV] max_features=auto, n_estimators=14 ..............................\n",
      "[CV]  max_features=auto, n_estimators=14, score=0.649573, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.626016, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.650407, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.666667, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.617886, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.661157, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.683333, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.558333, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.661017, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.635593, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=5 ...............................\n",
      "[CV]  max_features=log2, n_estimators=5, score=0.649573, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.642276, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.658537, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.658537, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.650407, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.669421, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.683333, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.583333, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.652542, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.635593, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=6 ...............................\n",
      "[CV]  max_features=log2, n_estimators=6, score=0.641026, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.658537, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.642276, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.674797, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.609756, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.652893, total=   0.0s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.683333, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.608333, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.686441, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.627119, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=7 ...............................\n",
      "[CV]  max_features=log2, n_estimators=7, score=0.666667, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.650407, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.674797, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.666667, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.617886, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.636364, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.675000, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.600000, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.661017, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.652542, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=8 ...............................\n",
      "[CV]  max_features=log2, n_estimators=8, score=0.641026, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.650407, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.699187, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.707317, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.609756, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.661157, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.675000, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.583333, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.686441, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.627119, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=9 ...............................\n",
      "[CV]  max_features=log2, n_estimators=9, score=0.658120, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.642276, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.658537, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.715447, total=   0.2s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.617886, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.652893, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.675000, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.600000, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.661017, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.644068, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=10 ..............................\n",
      "[CV]  max_features=log2, n_estimators=10, score=0.641026, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.691057, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.666667, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.707317, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.617886, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.661157, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.700000, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.600000, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.686441, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.610169, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=11 ..............................\n",
      "[CV]  max_features=log2, n_estimators=11, score=0.683761, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.682927, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.666667, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.715447, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.626016, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.669421, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.691667, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.608333, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.652542, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.627119, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=12 ..............................\n",
      "[CV]  max_features=log2, n_estimators=12, score=0.658120, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.674797, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.658537, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.723577, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.626016, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.685950, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.675000, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.600000, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.635593, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.627119, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=13 ..............................\n",
      "[CV]  max_features=log2, n_estimators=13, score=0.666667, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.674797, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.666667, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.715447, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.642276, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.661157, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.666667, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.583333, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.644068, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.652542, total=   0.1s\n",
      "[CV] max_features=log2, n_estimators=14 ..............................\n",
      "[CV]  max_features=log2, n_estimators=14, score=0.649573, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.691057, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.666667, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.585366, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.593496, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.669421, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.650000, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.558333, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.652542, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.559322, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=5 ...............................\n",
      "[CV]  max_features=None, n_estimators=5, score=0.675214, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.731707, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.650407, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.642276, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.601626, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.702479, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.666667, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.583333, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.635593, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.610169, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=6 ...............................\n",
      "[CV]  max_features=None, n_estimators=6, score=0.658120, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.731707, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.626016, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.650407, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.601626, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.685950, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.666667, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.566667, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.661017, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.576271, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=7 ...............................\n",
      "[CV]  max_features=None, n_estimators=7, score=0.666667, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.747967, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.658537, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.617886, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.601626, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.694215, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.683333, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.583333, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.661017, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.559322, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=8 ...............................\n",
      "[CV]  max_features=None, n_estimators=8, score=0.666667, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.723577, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.634146, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.617886, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.626016, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.685950, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.675000, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.591667, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.686441, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.559322, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=9 ...............................\n",
      "[CV]  max_features=None, n_estimators=9, score=0.658120, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.715447, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.650407, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.634146, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.601626, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.719008, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.675000, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.583333, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.694915, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.559322, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=10 ..............................\n",
      "[CV]  max_features=None, n_estimators=10, score=0.675214, total=   0.1s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.723577, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.666667, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.609756, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.609756, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.702479, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.700000, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.591667, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.694915, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.567797, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=11 ..............................\n",
      "[CV]  max_features=None, n_estimators=11, score=0.675214, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.707317, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.682927, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.617886, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.626016, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.727273, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.708333, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.583333, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.694915, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.567797, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=12 ..............................\n",
      "[CV]  max_features=None, n_estimators=12, score=0.700855, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.715447, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.658537, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.617886, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.626016, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.719008, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.675000, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.583333, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.703390, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.584746, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=13 ..............................\n",
      "[CV]  max_features=None, n_estimators=13, score=0.683761, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.731707, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.674797, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.609756, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.634146, total=   0.3s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.719008, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.683333, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.558333, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.711864, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.593220, total=   0.2s\n",
      "[CV] max_features=None, n_estimators=14 ..............................\n",
      "[CV]  max_features=None, n_estimators=14, score=0.692308, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:   33.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'max_features': ['auto', 'log2', None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    " \n",
    "# Use random forest to fit model\n",
    "search_modrf = sklearn.ensemble.RandomForestClassifier(criterion='gini', max_depth=None, random_state = 0)\n",
    "n_estimate = range(5,15)\n",
    "params = {'n_estimators':n_estimate, 'max_features': ['auto', 'log2', None]}\n",
    "\n",
    "search_rf = GridSearchCV(search_modrf, param_grid = params, cv = 10, verbose = 3)\n",
    "search_rf.fit(X_scale_train[:, 1:-1], y_train) # go back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict using fitted estimate\n",
    "# scale and process data\n",
    "\n",
    "ypred_rf = pandas.DataFrame({'pred_log': pandas.Series(search.predict(X_scale_test[:,1:-1]))})\n",
    "x_test_df = pandas.DataFrame(X_test)\n",
    "test_final_rf = pandas.concat([x_test_df, ypred_rf], axis=1)\n",
    "test_final_rf.to_csv(\"../clean/rf_pred_block.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
